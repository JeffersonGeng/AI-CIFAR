{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torchview import draw_graph\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "import timm\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the cifar10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(384),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(384),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Prepare dataset\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86,098,186 total parameters.\n",
      "86,098,186 total trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "net = timm.create_model(\"vit_base_patch16_384\", pretrained=True)\n",
    "net.head = nn.Linear(net.head.in_features, 10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4*6)\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} total trainable parameters.')\n",
    "# Move everything to the GPU\n",
    "net = net.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train:   2%|‚ñè         | 16/1000 [00:44<49:35,  3.02s/batch, ; Loss: 5.306 | Acc: 10.500% (84/800)]"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    pb = tqdm(total=1000, unit=\"batch\", leave=False, desc=f\"Epoch {epoch} Train\", position=0, file=sys.stdout)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Train with amp\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pb.postfix = '; Loss: %.3f | Acc: %.3f%% (%d/%d)'%(train_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "        pb.update(1)\n",
    "\n",
    "        #progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "           # % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    pb.close()\n",
    "    return train_loss/(batch_idx+1), correct/total\n",
    "\n",
    "def test(epoch):\n",
    "    pb = tqdm(total=200, unit=\"batch\", leave=False, desc=f\"Epoch {epoch} Test \", position=0, file=sys.stdout)\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            pb.postfix = '; Loss: %.3f | Acc: %.3f%% (%d/%d)'%(test_loss/(batch_idx+1), correct/total, correct, total)\n",
    "            pb.update(1)\n",
    "\n",
    "            #progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                #% (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    pb.close()\n",
    "    # Save checkpoint.\n",
    "    \n",
    "    #os.makedirs(\"log\", exist_ok=True)\n",
    "    return test_loss/(batch_idx+1), correct/total\n",
    "\n",
    "list_train_loss = []\n",
    "list_train_acc = []\n",
    "list_val_loss = []\n",
    "list_val_acc = []\n",
    "list_lr = []\n",
    "\n",
    "net.cuda()\n",
    "use_amp = True\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 50)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "\n",
    "for epoch in range(0, 50):\n",
    "    start = time.time()\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    val_loss, val_acc = test(epoch)\n",
    "    \n",
    "    scheduler.step() # step cosine scheduling\n",
    "\n",
    "    list_lr.append(optimizer.param_groups[0][\"lr\"])\n",
    "    list_train_loss.append(train_loss)\n",
    "    list_train_acc.append(train_acc)\n",
    "    list_val_loss.append(val_loss)\n",
    "    list_val_acc.append(val_acc)\n",
    "    content = f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, acc: {100*train_acc:.3f}%, val loss: {val_loss:.5f}, acc: {100*val_acc:.2f}%, time: {time.time()-start:.5f}s'\n",
    "    #with open(f'log/log_ViT_patch4.txt', 'a') as appender:\n",
    "    #    appender.write(content + \"\\n\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_acc = max(list_train_acc)\n",
    "best_val_acc = max(list_val_acc)\n",
    "best_train_loss = min(list_train_loss)\n",
    "best_val_loss = min(list_val_loss)\n",
    "plt.plot(list_train_loss)\n",
    "plt.plot(list_val_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "plt.plot(list_train_acc)\n",
    "plt.plot(list_val_acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "plt.plot(list_lr)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.show()\n",
    "print(f'Best train loss: {best_train_loss:.5f}, acc: {100*best_train_acc:.2f}%, Best val loss: {best_val_loss:.5f}, acc: {100*best_val_acc:.2f}%, test loss: {test_loss:.5f}, acc: {100*test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
